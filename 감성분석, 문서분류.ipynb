{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감성 분석 - https://mindscale.kr/course/python-text-classification/multinomial/\n",
    "\n",
    "문서 분류 - https://mindscale.kr/course/python-text-classification/doc-class-preprocess/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감성분석이란 문서에 나타난 긍부정의 감정 또는 태도를 분석하는 것입니다. 감정 분석에는 크게 사전에 의한 방법과 기계 학습에 의한 방법 2가지가 있다.\n",
    "\n",
    "사전에 의한 방법은 단어 중에 긍/부정의 감성을 나타내는 단어들을 사전으로 만들고, 문서에서 긍정 단어, 부정 단어의 비율을 계산해서 긍정 단어가 많으면 긍정으로, 부정 단어가 많으면 부정으로 판정하는 것. 감성 사전을 만들기 위해서는 높은 전문성과 많은 노력이 필요하다는 단점이 있다.\n",
    "\n",
    "기계학습에 의한 방법은 미리 긍/부정으로 분류된 문서들을 수집하여 기계학습 모형에 학습시키는 방법. 여기서는 아마존 핸드폰 리뷰 데이터를 기반으로 기계학습으로 감성 분석을 실시하는 방법을 알아보기로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip')\n",
    "with open('sentiment labelled sentences.zip', 'wb') as f:\n",
    "    f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "z = ZipFile('sentiment labelled sentences.zip')\n",
    "data = z.open('sentiment labelled sentences/amazon_cells_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  So there is no way for me to plug it in here i...  0\n",
       "1                        Good case, Excellent value.  1\n",
       "2                             Great for the jawbone.  1\n",
       "3  Tied to charger for conversations lasting more...  0\n",
       "4                                  The mic is great.  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data, sep=\"\\t\", header=None)\n",
    "df.head()\n",
    "\n",
    "# 0번 컬럼은 리뷰, 1번 컬럼에는 부정적 리뷰가 0, 긍정적 리뷰가 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어문서 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=1000, stop_words='english')\n",
    "tdm = cv.fit_transform(df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tdm\n",
    "y = df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3220 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형 모형을 통한 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dense(1): 1개의 예측(긍/부정)을 하는 선형 모형을 만듭니다.\n",
    "* input_shape=(1000,): 1000개의 단어를 입력 받습니다.\n",
    "* activation=: 시그모이드 활성화 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(1000,), activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 알고리즘(optimizer)는 adam을 사용합니다.\n",
    "* 손실함수(loss)는 교차 엔트로피(binary_crossentropy)를 사용합니다.\n",
    "* 보조적인 지표로 정확도(accurary)를 사용합니다. 정확도란 전체 사례 중 맞은 사례의 비율을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test) # 모형 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias = model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "word_weight = pd.DataFrame({\n",
    "    '단어': cv.get_feature_names(),\n",
    "    '가중치': weight.numpy().flat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weight.sort_values('가중치').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weight.sort_values('가중치', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'rec.motorcycles',\n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey'\n",
    "]\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF로 단어 문서 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1795, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "x_train = tfidf.fit_transform(newsgroups_train.data)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tfidf.transform(newsgroups_test.data)\n",
    "# 이때는 x_train과 똑같은 방식으로 단어 문서 행렬을 만들어야 하므로 fit_transform 대신에 transform을 사용.\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다항 분류 모형\n",
    "\n",
    "텐서플로를 사용하여 다항 분류 모형을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=3,\n",
    "    input_shape=(1000,),\n",
    "    kernel_regularizer=tf.keras.regularizers.l1_l2(0.003, 0.005),\n",
    "    activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.keras.layers.Dense() 로 층(완전 연결층)을 추가합니다. 세부 옵션은 다음과 같습니다.\n",
    "* units: 최종 출력의 개수를 유닛(unit)이라 합니다. 본 예시의 목표는 3가지 다항 분류이므로 최종 출력의 개수가 3 이 됩니다.\n",
    "* input_shape: 입력데이터의 크기 설정. 앞서 max_feature 개수를 1000으로 지정하였으므로 (1000,) 과 같이 튜플 형태로 입력합니다.\n",
    "* kernel_regularizer: 단어들의 가중치가 지나치게 커지지 않도록 정규화항을 추가해줍니다. l1_l2는 L1과 L2, 2가지 정규화를 모두 해줍니다. L1은 가중치의 절대값을 손실함수에 추가합니다. L2는 가중치의 제곱을 손실함수에 추가합니다. 어느 쪽이든 가중치가 커지면 손실이 커지므로, 가중치가 지나치게 커지는 것을 막아줍니다. l1_l2(0.003, 0.005)는 손실함수에서 L1항에 0.003을 곱하고 L2항 0.005를 곱하라는 뜻입니다. 여기서 0.003, 0.005와 같은 수치는 다양하게 바꿔보면서 성능이 가장 잘 나오는 수치를 선택합니다.\n",
    "* activation: 활성화 함수로 소프트맥스 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss: 모형의 손실함수는 sparse_categorical_crossentropy를 사용\n",
    "* optimizer: 여기에서 최적화 함수는 Adam (Adaptive Moment Estimation: 적응적 모멘트 추정) 을 사용하였습니다.\n",
    "* metrics: 모형 훈련 과정에서 기록할 요소를 정합니다. 여기에서는 정확도'accuracy'를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train.toarray(), y_train,\n",
    "                    epochs=30, callbacks=[tf.keras.callbacks.EarlyStopping()],\n",
    "                    validation_split=.3, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x_train.toarray(): 훈련 데이터\n",
    "* y_train: 지도 학습(supervised learning)에서 카테고리(label) 데이터\n",
    "* epochs: 에포크 횟수. 30일 경우 전체 데이터를 30번 학습합니다.\n",
    "* validation_split: 훈련(train)과 검증(validate) 데이터셋을 분리하는 비율. 여기서는 0.3 으로 설정하였고, 이는 1795개 중 30% 인 539개(반올림)를 검증용 데이터로 나눈다는 의미입니다.\n",
    "* callbacks: tf.keras.callbacks.EarlyStopping() 옵션은 모형의 validation loss가 3개 에포크 동안 낮아지지 않을 경우 학습을 멈추게 됩니다.\n",
    "* verbose: 모형 학습 중 출력되는 텍스트를 조절하는 옵션(verbosity). 0, 1, 2의 3개가 있으며verbose=0 일 경우, 아무런 문구도 출력되지 않습니다. verbose=2 일 경우, 진행상태를 나타내는 상태바(====)가 표시되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.title('LOSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "w, _ = model.weights\n",
    "weights = pd.DataFrame(w.numpy())\n",
    "\n",
    "weights.columns = ['motorcycle', 'baseball', 'hockey']\n",
    "weights['word'] = tfidf.get_feature_names()\n",
    "\n",
    "weights.sort_values('motorcycle', ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('adp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e121dbc38053d2982f45d1933ef59f2d15b105186547436b8f27260be9038f3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
